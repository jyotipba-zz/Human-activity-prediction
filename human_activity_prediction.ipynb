{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    " \n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7352, 561)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = pd.read_fwf('X_train.txt',header=None)  ## loading train data\n",
    "df_train.shape  ##(7352, 561)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>551</th>\n",
       "      <th>552</th>\n",
       "      <th>553</th>\n",
       "      <th>554</th>\n",
       "      <th>555</th>\n",
       "      <th>556</th>\n",
       "      <th>557</th>\n",
       "      <th>558</th>\n",
       "      <th>559</th>\n",
       "      <th>560</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>7352.000000</td>\n",
       "      <td>7352.000000</td>\n",
       "      <td>7352.000000</td>\n",
       "      <td>7352.000000</td>\n",
       "      <td>7352.000000</td>\n",
       "      <td>7352.000000</td>\n",
       "      <td>7352.000000</td>\n",
       "      <td>7352.000000</td>\n",
       "      <td>7352.000000</td>\n",
       "      <td>7352.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>7352.000000</td>\n",
       "      <td>7352.000000</td>\n",
       "      <td>7352.000000</td>\n",
       "      <td>7352.000000</td>\n",
       "      <td>7352.000000</td>\n",
       "      <td>7352.000000</td>\n",
       "      <td>7352.000000</td>\n",
       "      <td>7352.000000</td>\n",
       "      <td>7352.000000</td>\n",
       "      <td>7352.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.274488</td>\n",
       "      <td>-0.017695</td>\n",
       "      <td>-0.109141</td>\n",
       "      <td>-0.605438</td>\n",
       "      <td>-0.510938</td>\n",
       "      <td>-0.604754</td>\n",
       "      <td>-0.630512</td>\n",
       "      <td>-0.526907</td>\n",
       "      <td>-0.606150</td>\n",
       "      <td>-0.468604</td>\n",
       "      <td>...</td>\n",
       "      <td>0.125293</td>\n",
       "      <td>-0.307009</td>\n",
       "      <td>-0.625294</td>\n",
       "      <td>0.008684</td>\n",
       "      <td>0.002186</td>\n",
       "      <td>0.008726</td>\n",
       "      <td>-0.005981</td>\n",
       "      <td>-0.489547</td>\n",
       "      <td>0.058593</td>\n",
       "      <td>-0.056515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.070261</td>\n",
       "      <td>0.040811</td>\n",
       "      <td>0.056635</td>\n",
       "      <td>0.448734</td>\n",
       "      <td>0.502645</td>\n",
       "      <td>0.418687</td>\n",
       "      <td>0.424073</td>\n",
       "      <td>0.485942</td>\n",
       "      <td>0.414122</td>\n",
       "      <td>0.544547</td>\n",
       "      <td>...</td>\n",
       "      <td>0.250994</td>\n",
       "      <td>0.321011</td>\n",
       "      <td>0.307584</td>\n",
       "      <td>0.336787</td>\n",
       "      <td>0.448306</td>\n",
       "      <td>0.608303</td>\n",
       "      <td>0.477975</td>\n",
       "      <td>0.511807</td>\n",
       "      <td>0.297480</td>\n",
       "      <td>0.279122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.999873</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.995357</td>\n",
       "      <td>-0.999765</td>\n",
       "      <td>-0.976580</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.262975</td>\n",
       "      <td>-0.024863</td>\n",
       "      <td>-0.120993</td>\n",
       "      <td>-0.992754</td>\n",
       "      <td>-0.978129</td>\n",
       "      <td>-0.980233</td>\n",
       "      <td>-0.993591</td>\n",
       "      <td>-0.978162</td>\n",
       "      <td>-0.980251</td>\n",
       "      <td>-0.936219</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.023692</td>\n",
       "      <td>-0.542602</td>\n",
       "      <td>-0.845573</td>\n",
       "      <td>-0.121527</td>\n",
       "      <td>-0.289549</td>\n",
       "      <td>-0.482273</td>\n",
       "      <td>-0.376341</td>\n",
       "      <td>-0.812065</td>\n",
       "      <td>-0.017885</td>\n",
       "      <td>-0.143414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.277193</td>\n",
       "      <td>-0.017219</td>\n",
       "      <td>-0.108676</td>\n",
       "      <td>-0.946196</td>\n",
       "      <td>-0.851897</td>\n",
       "      <td>-0.859365</td>\n",
       "      <td>-0.950709</td>\n",
       "      <td>-0.857328</td>\n",
       "      <td>-0.857143</td>\n",
       "      <td>-0.881637</td>\n",
       "      <td>...</td>\n",
       "      <td>0.134000</td>\n",
       "      <td>-0.343685</td>\n",
       "      <td>-0.711692</td>\n",
       "      <td>0.009509</td>\n",
       "      <td>0.008943</td>\n",
       "      <td>0.008735</td>\n",
       "      <td>-0.000368</td>\n",
       "      <td>-0.709417</td>\n",
       "      <td>0.182071</td>\n",
       "      <td>0.003181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.288461</td>\n",
       "      <td>-0.010783</td>\n",
       "      <td>-0.097794</td>\n",
       "      <td>-0.242813</td>\n",
       "      <td>-0.034231</td>\n",
       "      <td>-0.262415</td>\n",
       "      <td>-0.292680</td>\n",
       "      <td>-0.066701</td>\n",
       "      <td>-0.265671</td>\n",
       "      <td>-0.017129</td>\n",
       "      <td>...</td>\n",
       "      <td>0.289096</td>\n",
       "      <td>-0.126979</td>\n",
       "      <td>-0.503878</td>\n",
       "      <td>0.150865</td>\n",
       "      <td>0.292861</td>\n",
       "      <td>0.506187</td>\n",
       "      <td>0.359368</td>\n",
       "      <td>-0.509079</td>\n",
       "      <td>0.248353</td>\n",
       "      <td>0.107659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.916238</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.967664</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.946700</td>\n",
       "      <td>0.989538</td>\n",
       "      <td>0.956845</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.998702</td>\n",
       "      <td>0.996078</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.478157</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 561 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               0            1            2            3            4    \\\n",
       "count  7352.000000  7352.000000  7352.000000  7352.000000  7352.000000   \n",
       "mean      0.274488    -0.017695    -0.109141    -0.605438    -0.510938   \n",
       "std       0.070261     0.040811     0.056635     0.448734     0.502645   \n",
       "min      -1.000000    -1.000000    -1.000000    -1.000000    -0.999873   \n",
       "25%       0.262975    -0.024863    -0.120993    -0.992754    -0.978129   \n",
       "50%       0.277193    -0.017219    -0.108676    -0.946196    -0.851897   \n",
       "75%       0.288461    -0.010783    -0.097794    -0.242813    -0.034231   \n",
       "max       1.000000     1.000000     1.000000     1.000000     0.916238   \n",
       "\n",
       "               5            6            7            8            9    \\\n",
       "count  7352.000000  7352.000000  7352.000000  7352.000000  7352.000000   \n",
       "mean     -0.604754    -0.630512    -0.526907    -0.606150    -0.468604   \n",
       "std       0.418687     0.424073     0.485942     0.414122     0.544547   \n",
       "min      -1.000000    -1.000000    -1.000000    -1.000000    -1.000000   \n",
       "25%      -0.980233    -0.993591    -0.978162    -0.980251    -0.936219   \n",
       "50%      -0.859365    -0.950709    -0.857328    -0.857143    -0.881637   \n",
       "75%      -0.262415    -0.292680    -0.066701    -0.265671    -0.017129   \n",
       "max       1.000000     1.000000     0.967664     1.000000     1.000000   \n",
       "\n",
       "          ...               551          552          553          554  \\\n",
       "count     ...       7352.000000  7352.000000  7352.000000  7352.000000   \n",
       "mean      ...          0.125293    -0.307009    -0.625294     0.008684   \n",
       "std       ...          0.250994     0.321011     0.307584     0.336787   \n",
       "min       ...         -1.000000    -0.995357    -0.999765    -0.976580   \n",
       "25%       ...         -0.023692    -0.542602    -0.845573    -0.121527   \n",
       "50%       ...          0.134000    -0.343685    -0.711692     0.009509   \n",
       "75%       ...          0.289096    -0.126979    -0.503878     0.150865   \n",
       "max       ...          0.946700     0.989538     0.956845     1.000000   \n",
       "\n",
       "               555          556          557          558          559  \\\n",
       "count  7352.000000  7352.000000  7352.000000  7352.000000  7352.000000   \n",
       "mean      0.002186     0.008726    -0.005981    -0.489547     0.058593   \n",
       "std       0.448306     0.608303     0.477975     0.511807     0.297480   \n",
       "min      -1.000000    -1.000000    -1.000000    -1.000000    -1.000000   \n",
       "25%      -0.289549    -0.482273    -0.376341    -0.812065    -0.017885   \n",
       "50%       0.008943     0.008735    -0.000368    -0.709417     0.182071   \n",
       "75%       0.292861     0.506187     0.359368    -0.509079     0.248353   \n",
       "max       1.000000     0.998702     0.996078     1.000000     0.478157   \n",
       "\n",
       "               560  \n",
       "count  7352.000000  \n",
       "mean     -0.056515  \n",
       "std       0.279122  \n",
       "min      -1.000000  \n",
       "25%      -0.143414  \n",
       "50%       0.003181  \n",
       "75%       0.107659  \n",
       "max       1.000000  \n",
       "\n",
       "[8 rows x 561 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.describe()   ## Provides the summary staistics of numrical vaiables\n",
    "##  all the variables are numrical "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "561"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chk_missing_value=df_train.apply(lambda x: sum(x.isnull()), axis=0)\n",
    "sum(chk_missing_value==0)  ## NO variable has missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "print(df_train.isnull().any().any())\n",
    "##print(test.isnull().any().any())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1 tBodyAcc-mean()-X</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2 tBodyAcc-mean()-Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3 tBodyAcc-mean()-Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4 tBodyAcc-std()-X</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5 tBodyAcc-std()-Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6 tBodyAcc-std()-Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7 tBodyAcc-mad()-X</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8 tBodyAcc-mad()-Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9 tBodyAcc-mad()-Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10 tBodyAcc-max()-X</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     0\n",
       "0  1 tBodyAcc-mean()-X\n",
       "1  2 tBodyAcc-mean()-Y\n",
       "2  3 tBodyAcc-mean()-Z\n",
       "3   4 tBodyAcc-std()-X\n",
       "4   5 tBodyAcc-std()-Y\n",
       "5   6 tBodyAcc-std()-Z\n",
       "6   7 tBodyAcc-mad()-X\n",
       "7   8 tBodyAcc-mad()-Y\n",
       "8   9 tBodyAcc-mad()-Z\n",
       "9  10 tBodyAcc-max()-X"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features=pd.read_fwf(\"features.txt\",header=None)  ## Feature names\n",
    "features.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_train=pd.read_fwf(\"y_train.txt\",header=None)  ## train class label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6    1407\n",
       "5    1374\n",
       "4    1286\n",
       "1    1226\n",
       "2    1073\n",
       "3     986\n",
       "Name: 0, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[0].value_counts() ## Viewing the  class distribution in training set data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>551</th>\n",
       "      <th>552</th>\n",
       "      <th>553</th>\n",
       "      <th>554</th>\n",
       "      <th>555</th>\n",
       "      <th>556</th>\n",
       "      <th>557</th>\n",
       "      <th>558</th>\n",
       "      <th>559</th>\n",
       "      <th>560</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2947.000000</td>\n",
       "      <td>2947.000000</td>\n",
       "      <td>2947.000000</td>\n",
       "      <td>2947.000000</td>\n",
       "      <td>2947.000000</td>\n",
       "      <td>2947.000000</td>\n",
       "      <td>2947.000000</td>\n",
       "      <td>2947.000000</td>\n",
       "      <td>2947.000000</td>\n",
       "      <td>2947.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>2947.000000</td>\n",
       "      <td>2947.000000</td>\n",
       "      <td>2947.000000</td>\n",
       "      <td>2947.000000</td>\n",
       "      <td>2947.000000</td>\n",
       "      <td>2947.000000</td>\n",
       "      <td>2947.000000</td>\n",
       "      <td>2947.000000</td>\n",
       "      <td>2947.000000</td>\n",
       "      <td>2947.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.275860</td>\n",
       "      <td>-0.017863</td>\n",
       "      <td>-0.108386</td>\n",
       "      <td>-0.613635</td>\n",
       "      <td>-0.508330</td>\n",
       "      <td>-0.633797</td>\n",
       "      <td>-0.641278</td>\n",
       "      <td>-0.522676</td>\n",
       "      <td>-0.637038</td>\n",
       "      <td>-0.462063</td>\n",
       "      <td>...</td>\n",
       "      <td>0.130236</td>\n",
       "      <td>-0.277593</td>\n",
       "      <td>-0.598756</td>\n",
       "      <td>0.005264</td>\n",
       "      <td>0.003799</td>\n",
       "      <td>0.040029</td>\n",
       "      <td>-0.017298</td>\n",
       "      <td>-0.513923</td>\n",
       "      <td>0.074886</td>\n",
       "      <td>-0.048720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.051413</td>\n",
       "      <td>0.025745</td>\n",
       "      <td>0.042747</td>\n",
       "      <td>0.412597</td>\n",
       "      <td>0.494269</td>\n",
       "      <td>0.362699</td>\n",
       "      <td>0.385199</td>\n",
       "      <td>0.479899</td>\n",
       "      <td>0.357753</td>\n",
       "      <td>0.523916</td>\n",
       "      <td>...</td>\n",
       "      <td>0.231018</td>\n",
       "      <td>0.317245</td>\n",
       "      <td>0.311042</td>\n",
       "      <td>0.336147</td>\n",
       "      <td>0.445077</td>\n",
       "      <td>0.634989</td>\n",
       "      <td>0.501311</td>\n",
       "      <td>0.509205</td>\n",
       "      <td>0.324300</td>\n",
       "      <td>0.241467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.022332</td>\n",
       "      <td>-0.362884</td>\n",
       "      <td>-0.576184</td>\n",
       "      <td>-0.999606</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.998955</td>\n",
       "      <td>-0.999417</td>\n",
       "      <td>-0.999914</td>\n",
       "      <td>-0.998899</td>\n",
       "      <td>-0.952357</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.785543</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.993402</td>\n",
       "      <td>-0.998898</td>\n",
       "      <td>-0.991096</td>\n",
       "      <td>-0.984195</td>\n",
       "      <td>-0.913704</td>\n",
       "      <td>-0.949228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.262273</td>\n",
       "      <td>-0.024961</td>\n",
       "      <td>-0.121162</td>\n",
       "      <td>-0.990914</td>\n",
       "      <td>-0.973664</td>\n",
       "      <td>-0.976122</td>\n",
       "      <td>-0.992333</td>\n",
       "      <td>-0.974131</td>\n",
       "      <td>-0.975352</td>\n",
       "      <td>-0.934447</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.008433</td>\n",
       "      <td>-0.517494</td>\n",
       "      <td>-0.829593</td>\n",
       "      <td>-0.130541</td>\n",
       "      <td>-0.282600</td>\n",
       "      <td>-0.518924</td>\n",
       "      <td>-0.428375</td>\n",
       "      <td>-0.829722</td>\n",
       "      <td>0.022140</td>\n",
       "      <td>-0.098485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.277146</td>\n",
       "      <td>-0.016967</td>\n",
       "      <td>-0.108458</td>\n",
       "      <td>-0.931214</td>\n",
       "      <td>-0.790972</td>\n",
       "      <td>-0.827534</td>\n",
       "      <td>-0.937664</td>\n",
       "      <td>-0.799907</td>\n",
       "      <td>-0.817005</td>\n",
       "      <td>-0.852659</td>\n",
       "      <td>...</td>\n",
       "      <td>0.142676</td>\n",
       "      <td>-0.311023</td>\n",
       "      <td>-0.683672</td>\n",
       "      <td>0.005188</td>\n",
       "      <td>0.006767</td>\n",
       "      <td>0.047113</td>\n",
       "      <td>-0.026726</td>\n",
       "      <td>-0.729648</td>\n",
       "      <td>0.181563</td>\n",
       "      <td>-0.010671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.288372</td>\n",
       "      <td>-0.010143</td>\n",
       "      <td>-0.097123</td>\n",
       "      <td>-0.267395</td>\n",
       "      <td>-0.105919</td>\n",
       "      <td>-0.311432</td>\n",
       "      <td>-0.321719</td>\n",
       "      <td>-0.133488</td>\n",
       "      <td>-0.322771</td>\n",
       "      <td>-0.009965</td>\n",
       "      <td>...</td>\n",
       "      <td>0.288320</td>\n",
       "      <td>-0.083559</td>\n",
       "      <td>-0.458332</td>\n",
       "      <td>0.146200</td>\n",
       "      <td>0.288113</td>\n",
       "      <td>0.622151</td>\n",
       "      <td>0.394387</td>\n",
       "      <td>-0.545939</td>\n",
       "      <td>0.260252</td>\n",
       "      <td>0.092373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.671887</td>\n",
       "      <td>0.246106</td>\n",
       "      <td>0.494114</td>\n",
       "      <td>0.465299</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.489703</td>\n",
       "      <td>0.439657</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.427958</td>\n",
       "      <td>0.786436</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.998898</td>\n",
       "      <td>0.986347</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.833180</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.973113</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 561 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               0            1            2            3            4    \\\n",
       "count  2947.000000  2947.000000  2947.000000  2947.000000  2947.000000   \n",
       "mean      0.275860    -0.017863    -0.108386    -0.613635    -0.508330   \n",
       "std       0.051413     0.025745     0.042747     0.412597     0.494269   \n",
       "min       0.022332    -0.362884    -0.576184    -0.999606    -1.000000   \n",
       "25%       0.262273    -0.024961    -0.121162    -0.990914    -0.973664   \n",
       "50%       0.277146    -0.016967    -0.108458    -0.931214    -0.790972   \n",
       "75%       0.288372    -0.010143    -0.097123    -0.267395    -0.105919   \n",
       "max       0.671887     0.246106     0.494114     0.465299     1.000000   \n",
       "\n",
       "               5            6            7            8            9    \\\n",
       "count  2947.000000  2947.000000  2947.000000  2947.000000  2947.000000   \n",
       "mean     -0.633797    -0.641278    -0.522676    -0.637038    -0.462063   \n",
       "std       0.362699     0.385199     0.479899     0.357753     0.523916   \n",
       "min      -0.998955    -0.999417    -0.999914    -0.998899    -0.952357   \n",
       "25%      -0.976122    -0.992333    -0.974131    -0.975352    -0.934447   \n",
       "50%      -0.827534    -0.937664    -0.799907    -0.817005    -0.852659   \n",
       "75%      -0.311432    -0.321719    -0.133488    -0.322771    -0.009965   \n",
       "max       0.489703     0.439657     1.000000     0.427958     0.786436   \n",
       "\n",
       "          ...               551          552          553          554  \\\n",
       "count     ...       2947.000000  2947.000000  2947.000000  2947.000000   \n",
       "mean      ...          0.130236    -0.277593    -0.598756     0.005264   \n",
       "std       ...          0.231018     0.317245     0.311042     0.336147   \n",
       "min       ...         -0.785543    -1.000000    -1.000000    -1.000000   \n",
       "25%       ...         -0.008433    -0.517494    -0.829593    -0.130541   \n",
       "50%       ...          0.142676    -0.311023    -0.683672     0.005188   \n",
       "75%       ...          0.288320    -0.083559    -0.458332     0.146200   \n",
       "max       ...          1.000000     1.000000     1.000000     0.998898   \n",
       "\n",
       "               555          556          557          558          559  \\\n",
       "count  2947.000000  2947.000000  2947.000000  2947.000000  2947.000000   \n",
       "mean      0.003799     0.040029    -0.017298    -0.513923     0.074886   \n",
       "std       0.445077     0.634989     0.501311     0.509205     0.324300   \n",
       "min      -0.993402    -0.998898    -0.991096    -0.984195    -0.913704   \n",
       "25%      -0.282600    -0.518924    -0.428375    -0.829722     0.022140   \n",
       "50%       0.006767     0.047113    -0.026726    -0.729648     0.181563   \n",
       "75%       0.288113     0.622151     0.394387    -0.545939     0.260252   \n",
       "max       0.986347     1.000000     1.000000     0.833180     1.000000   \n",
       "\n",
       "               560  \n",
       "count  2947.000000  \n",
       "mean     -0.048720  \n",
       "std       0.241467  \n",
       "min      -0.949228  \n",
       "25%      -0.098485  \n",
       "50%      -0.010671  \n",
       "75%       0.092373  \n",
       "max       0.973113  \n",
       "\n",
       "[8 rows x 561 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test = pd.read_fwf('X_test.txt',header=None)  ## loading test data\n",
    "df_test.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2947, 561)\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "print(df_test.shape)\n",
    "print(df_test.isnull().any().any())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_test=pd.read_fwf(\"y_test.txt\",header=None)  ## test class label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6    537\n",
       "5    532\n",
       "1    496\n",
       "4    491\n",
       "2    471\n",
       "3    420\n",
       "Name: 0, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test[0].value_counts()\n",
    "\n",
    "## This shows that the classification problem is multiclass and also data set is balanced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##Machine learning part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "nb_model = GaussianNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GaussianNB()"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_model.fit(df_train, y_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.77061418391584657"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_model.score(df_test,y_test) ## model accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, error_score='raise',\n",
       "       estimator=SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape=None, degree=3, gamma='auto', kernel='rbf',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False),\n",
       "       fit_params={}, iid=True, n_jobs=1,\n",
       "       param_grid=[{'C': [1, 10, 100, 1000], 'kernel': ['rbf'], 'gamma': [0.1, 0.01, 0.001, 0.0001]}],\n",
       "       pre_dispatch='2*n_jobs', refit=True, scoring=None, verbose=0)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Support vector machine, using 3 fold cross validation for parameter tuning \n",
    "\n",
    "\n",
    "\n",
    "tuned_parameters = [{'kernel': ['rbf'], 'gamma': [0.1,0.01,0.001, 0.0001],\n",
    "                     'C': [1, 10, 100, 1000]}]\n",
    "\n",
    "svm = GridSearchCV(SVC(), tuned_parameters, cv=3)\n",
    "\n",
    "\n",
    "\n",
    "svm.fit(df_train, y_train[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 1000, 'gamma': 0.01, 'kernel': 'rbf'}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm.best_params_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.96267390566677979"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm.score(df_test,y_test)  ## Accuracy of SVM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PCA(copy=True, n_components=None, whiten=False)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "  ## Principl component analysis for dimensionality reduction. \n",
    "    \n",
    "\n",
    "pca_train= PCA()\n",
    "pca_train.fit(df_train)\n",
    "\n",
    "##pca_train = pca_train.transform(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "var= pca_train.explained_variance_ratio_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 67.45,  71.58,  73.45,  75.14,  76.41,  77.59,  78.66,  79.63,\n",
       "        80.49,  81.25,  81.92,  82.5 ,  83.06,  83.56,  84.03,  84.5 ,\n",
       "        84.93,  85.36,  85.77,  86.16,  86.54,  86.89,  87.23,  87.56,\n",
       "        87.88,  88.18,  88.47,  88.76,  89.04,  89.32,  89.58,  89.84,\n",
       "        90.08,  90.31,  90.53,  90.75,  90.96,  91.17,  91.37,  91.56,\n",
       "        91.75,  91.93,  92.11,  92.28,  92.44,  92.6 ,  92.76,  92.91,\n",
       "        93.06,  93.2 ,  93.34,  93.48,  93.61,  93.74,  93.86,  93.98,\n",
       "        94.1 ,  94.21,  94.32,  94.43,  94.53,  94.63,  94.73,  94.83])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now calculating the cumulative Variance explained\n",
    "cum_var=np.cumsum(np.round(pca_train.explained_variance_ratio_, decimals=4)*100)\n",
    "cum_var[1:65]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x224b7395ac8>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYwAAAEPCAYAAABRHfM8AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x224b68fd4a8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.plot(cum_var)\n",
    "plt.ylabel(\"Cumulative variance explained\")\n",
    "plt.xlabel(\"principal compoents\")\n",
    "\n",
    "## It  65 componets explains alomost 95 percent of variance in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2947, 65)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## NOw performing pca with 65 components\n",
    "\n",
    "pca = PCA(n_components=65)\n",
    "pca.fit(df_train)\n",
    "\n",
    "pca_train = pca.transform(df_train)\n",
    "pca_test = pca.transform(df_test)\n",
    "\n",
    "pca_test.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nb_model_pca = GaussianNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.87580590430946725"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_model_pca.fit(pca_train, y_train[0])\n",
    "nb_model_pca.score(pca_test,y_test) ## model accuracy of naive bayes after reducing dimension\n",
    "\n",
    "## We see the accuracy improved "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, error_score='raise',\n",
       "       estimator=SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape=None, degree=3, gamma='auto', kernel='rbf',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False),\n",
       "       fit_params={}, iid=True, n_jobs=1,\n",
       "       param_grid=[{'C': [1, 10, 100, 1000], 'kernel': ['rbf'], 'gamma': [0.1, 0.01, 0.001, 0.0001]}],\n",
       "       pre_dispatch='2*n_jobs', refit=True, scoring=None, verbose=0)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Support vector machine on dimension reduced data , using 3 fold cross validation for parameter tuning  \n",
    "\n",
    "\n",
    "\n",
    "tuned_parameters_pca = [{'kernel': ['rbf'], 'gamma': [0.1,0.01,0.001, 0.0001],\n",
    "                     'C': [1, 10, 100, 1000]}]\n",
    "\n",
    "svm_pca = GridSearchCV(SVC(), tuned_parameters_pca, cv=3)\n",
    "\n",
    "\n",
    "\n",
    "svm_pca.fit(pca_train, y_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 1000, 'gamma': 0.01, 'kernel': 'rbf'}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_pca.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.94061757719714967"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "svm_pca.score(pca_test,y_test) \n",
    "\n",
    "## SVM model accuracy after reducing dimension using PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Random forests\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rf_model = RandomForestClassifier(n_estimators=501, oob_score=True)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.92975907702748561"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_model.fit(df_train, y_train[0])\n",
    "rf_model.score(df_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Viewing the feature importance\n",
    "feat_importances = rf_model.feature_importances_\n",
    "indices = np.argsort(feat_importances)[::-1]##  sorting the feature indices in descending order\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.03085079,  0.0305172 ,  0.02922103,  0.02915389,  0.02748138,\n",
       "        0.02553096,  0.02387736,  0.02342588,  0.02290399,  0.01623984,\n",
       "        0.01386023,  0.01081371,  0.01048121,  0.00960825,  0.00947361,\n",
       "        0.00943038,  0.00930744,  0.00922816,  0.00873158,  0.00825693,\n",
       "        0.00822697,  0.00811096,  0.00788929,  0.00769559,  0.00755556,\n",
       "        0.00745049,  0.00718778,  0.00714661,  0.0070203 ,  0.00690196,\n",
       "        0.00681315,  0.00681188,  0.00680106,  0.00676528,  0.00671638,\n",
       "        0.00653898,  0.00630725,  0.00621867,  0.00609059,  0.00598086,\n",
       "        0.00591948,  0.00591877,  0.00583141,  0.00574741,  0.00570512,\n",
       "        0.00566221,  0.00565954,  0.00556315,  0.0055117 ,  0.00549182,\n",
       "        0.00546386,  0.00542529,  0.00513609,  0.00495163,  0.00490793,\n",
       "        0.00478287,  0.00478114,  0.0047006 ,  0.0045426 ,  0.00451585,\n",
       "        0.00445008,  0.00442922,  0.00421174,  0.00414231,  0.00404772,\n",
       "        0.00404103,  0.00400864,  0.00397871,  0.00392698,  0.00389891,\n",
       "        0.00376593,  0.0036683 ,  0.00362185,  0.00353345,  0.00348567,\n",
       "        0.00341235,  0.00341159,  0.00333805,  0.00331559,  0.00329269,\n",
       "        0.00317903,  0.00316226,  0.0031351 ,  0.00312701,  0.00308091,\n",
       "        0.0030733 ,  0.00306616,  0.00302634,  0.00300067,  0.00293022,\n",
       "        0.00292467,  0.00291978,  0.00290089,  0.00286779,  0.00284675,\n",
       "        0.00283796,  0.00282287,  0.0027909 ,  0.00277059,  0.00273094,\n",
       "        0.00270966,  0.00253204,  0.0025089 ,  0.00246897,  0.0024487 ,\n",
       "        0.00232896,  0.00230583,  0.00230118,  0.00226141,  0.00226121,\n",
       "        0.00216982,  0.00216297,  0.00213322,  0.00212819,  0.00211639,\n",
       "        0.00208552,  0.00208223,  0.00207875,  0.00207238,  0.00203357,\n",
       "        0.00202975,  0.00202595,  0.0020078 ,  0.00199218,  0.00194735,\n",
       "        0.0019325 ,  0.00191525,  0.0019104 ,  0.00188759,  0.00187188,\n",
       "        0.00186806,  0.00183169,  0.00182904,  0.00181911,  0.00181869,\n",
       "        0.00169789,  0.00160724,  0.00156172,  0.00155455,  0.00151122,\n",
       "        0.0014994 ,  0.00148174,  0.00148141,  0.00146501,  0.0014614 ,\n",
       "        0.00144572,  0.00141988,  0.00138523,  0.00137877,  0.0013714 ,\n",
       "        0.001358  ,  0.00135054,  0.00132044,  0.00131859,  0.0013178 ,\n",
       "        0.00131406,  0.00130747,  0.00127572,  0.00119672,  0.0011507 ,\n",
       "        0.00113201,  0.00111525,  0.00108032,  0.0010525 ,  0.0010291 ,\n",
       "        0.00101326,  0.00101154,  0.00100899,  0.00099803,  0.00099747,\n",
       "        0.00099289,  0.00093133,  0.00093063,  0.00092871,  0.00092513,\n",
       "        0.00090976,  0.00090954,  0.00088692,  0.0008863 ,  0.00087245,\n",
       "        0.00087016,  0.0008674 ,  0.00085102,  0.00084537,  0.00083282,\n",
       "        0.00082411,  0.00082336,  0.00080405,  0.00080372,  0.00079574,\n",
       "        0.00079071,  0.00078641,  0.00078349,  0.00077775,  0.00077293,\n",
       "        0.00075018,  0.00074869,  0.00074115,  0.00073887,  0.00073777,\n",
       "        0.00073665,  0.00073557,  0.00073543,  0.00073497,  0.00073015,\n",
       "        0.00072462,  0.00072205,  0.00072055,  0.00071632,  0.00071434,\n",
       "        0.00071237,  0.00069956,  0.00069346,  0.0006859 ,  0.00068291,\n",
       "        0.00068223,  0.00067795,  0.00067578,  0.00065322,  0.00064793,\n",
       "        0.0006466 ,  0.00064495,  0.00064284,  0.00064184,  0.00064039,\n",
       "        0.00063589,  0.00063118,  0.00062856,  0.0006283 ,  0.00062427,\n",
       "        0.00062248,  0.00062229,  0.00062152,  0.00061829,  0.00060965,\n",
       "        0.00060305,  0.00058667,  0.0005815 ,  0.00057209,  0.00057022,\n",
       "        0.0005686 ,  0.00056774,  0.00056652,  0.00055975,  0.00055864,\n",
       "        0.000557  ,  0.00054862,  0.00054137,  0.00052862,  0.00052324,\n",
       "        0.00052081,  0.00051869,  0.00051797,  0.00051593,  0.00051537,\n",
       "        0.00051165,  0.00051143,  0.00051126,  0.00051055,  0.00050383,\n",
       "        0.00050218,  0.00049589,  0.00049257,  0.00049253,  0.00049196,\n",
       "        0.00049099,  0.00049012,  0.00048524,  0.00048399,  0.00047689,\n",
       "        0.00047017,  0.00046869,  0.00046357,  0.00045877,  0.00045627,\n",
       "        0.00045538,  0.00045098,  0.00044881,  0.00044757,  0.00044302,\n",
       "        0.00044286,  0.00044244,  0.00043458,  0.00043318,  0.00042955,\n",
       "        0.00042954,  0.00042872,  0.00041234,  0.00040792,  0.00040648,\n",
       "        0.00040562,  0.00040255,  0.00039944,  0.00039581,  0.0003956 ,\n",
       "        0.00039399,  0.00039267,  0.00039047,  0.00038632,  0.0003848 ,\n",
       "        0.0003846 ,  0.00038264,  0.00038157,  0.00038076,  0.00037959,\n",
       "        0.00037788,  0.00037756,  0.00037668,  0.0003757 ,  0.00037496,\n",
       "        0.00037387,  0.00037357,  0.00037259,  0.00037154,  0.00037095,\n",
       "        0.00037003,  0.00036988,  0.00036891,  0.00036473,  0.00035762,\n",
       "        0.00035736,  0.00035517,  0.00035268,  0.00035265,  0.00035014,\n",
       "        0.00035002,  0.00034986,  0.00034101,  0.00033977,  0.00033665,\n",
       "        0.00033381,  0.0003321 ,  0.00033162,  0.00032921,  0.00032831,\n",
       "        0.00032683,  0.00032591,  0.00032513,  0.00032212,  0.0003191 ,\n",
       "        0.0003181 ,  0.00031738,  0.00031601,  0.00031439,  0.00031379,\n",
       "        0.0003131 ,  0.00031263,  0.00031142,  0.00031066,  0.00030935,\n",
       "        0.00030915,  0.00030902,  0.00030489,  0.00030263,  0.00030028,\n",
       "        0.00029983,  0.00029631,  0.00029379,  0.00029271,  0.00029186,\n",
       "        0.00029156,  0.00029047,  0.00028962,  0.00028931,  0.00028875,\n",
       "        0.00028856,  0.00028774,  0.00028642,  0.00028421,  0.0002824 ,\n",
       "        0.00028167,  0.00028092,  0.00028029,  0.00027902,  0.00027717,\n",
       "        0.00027476,  0.00027289,  0.00027242,  0.00027181,  0.00027168,\n",
       "        0.00027126,  0.00027109,  0.00027001,  0.00026626,  0.00026597,\n",
       "        0.00026574,  0.00026543,  0.00026514,  0.00026278,  0.00026268,\n",
       "        0.00026226,  0.00026159,  0.00026084,  0.00026026,  0.00025923,\n",
       "        0.00025857,  0.00025855,  0.00025767,  0.00025613,  0.00025455,\n",
       "        0.00025353,  0.00025307,  0.00025236,  0.0002511 ,  0.00025086,\n",
       "        0.00025055,  0.00025037,  0.00024988,  0.00024683,  0.00024656,\n",
       "        0.000246  ,  0.00024199,  0.00023945,  0.00023918,  0.00023906,\n",
       "        0.00023591,  0.000235  ,  0.00023487,  0.00023443,  0.00023431,\n",
       "        0.00023198,  0.00023063,  0.00023041,  0.00022571,  0.00022511,\n",
       "        0.00022463,  0.00022237,  0.00022194,  0.00022163,  0.0002216 ,\n",
       "        0.00022031,  0.00022003,  0.00021824,  0.00021632,  0.00021517,\n",
       "        0.00021295,  0.00021136,  0.00021033,  0.00020993,  0.00020983,\n",
       "        0.00020904,  0.00020841,  0.00020802,  0.0002073 ,  0.00020685,\n",
       "        0.00020434,  0.00020297,  0.00020241,  0.00020192,  0.00020184,\n",
       "        0.00019945,  0.00019819,  0.00019765,  0.00019749,  0.00019668,\n",
       "        0.00019634,  0.00019612,  0.00019569,  0.00019552,  0.00019482,\n",
       "        0.00019402,  0.00019384,  0.00019381,  0.00019372,  0.00019323,\n",
       "        0.00019191,  0.00019183,  0.00019111,  0.00019036,  0.00018989,\n",
       "        0.00018954,  0.00018953,  0.00018869,  0.00018707,  0.00018577,\n",
       "        0.00018563,  0.0001841 ,  0.00018331,  0.00018216,  0.000182  ,\n",
       "        0.00018083,  0.00018059,  0.00018011,  0.00017967,  0.00017953,\n",
       "        0.00017926,  0.00017871,  0.00017665,  0.00017582,  0.00017556,\n",
       "        0.00017503,  0.00017434,  0.00017318,  0.00017281,  0.0001726 ,\n",
       "        0.00017176,  0.00017116,  0.00017081,  0.00016773,  0.00016707,\n",
       "        0.00016651,  0.00016603,  0.00016576,  0.00016469,  0.00016386,\n",
       "        0.00016371,  0.00016327,  0.00016318,  0.00016283,  0.00016228,\n",
       "        0.00016207,  0.00016144,  0.00016019,  0.00015842,  0.00015807,\n",
       "        0.00015797,  0.00015698,  0.0001554 ,  0.00015449,  0.00015271,\n",
       "        0.00015199,  0.00015047,  0.00014905,  0.00014652,  0.00014549,\n",
       "        0.00014547,  0.00014544,  0.00014444,  0.00014405,  0.00014398,\n",
       "        0.0001435 ,  0.00014323,  0.00014222,  0.00014201,  0.00014141,\n",
       "        0.00013919,  0.00013779,  0.00013776,  0.0001369 ,  0.0001362 ,\n",
       "        0.00013533,  0.00013417,  0.00013413,  0.00013081,  0.00013049,\n",
       "        0.00013021,  0.00012839,  0.00012835,  0.00012723,  0.00012534,\n",
       "        0.0001225 ,  0.00012004,  0.00011923,  0.0001187 ,  0.00011765,\n",
       "        0.00011642,  0.00011404,  0.00011306,  0.00010963,  0.00010686,\n",
       "        0.0001018 ])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feat_importances[indices]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x15b85b47b38>]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAEACAYAAABYq7oeAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x15b85ae1c18>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(feat_importances[indices])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Now, running random forest with 100 most informative features\n",
    "\n",
    "\n",
    "reduced_df_train=df_train.iloc[ :,indices[0:100] ]\n",
    "reduced_df_test=df_test.iloc[ :,indices[0:100] ]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=501, n_jobs=1,\n",
       "            oob_score=True, random_state=None, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reduced_rf = RandomForestClassifier(n_estimators=501, oob_score=True)\n",
    "\n",
    "\n",
    "reduced_rf.fit(reduced_df_train, y_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.90668476416694943"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reduced_rf.score(reduced_df_test,y_test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.90668476416694943"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "predicted = reduced_rf.predict(reduced_df_test)\n",
    "accuracy = accuracy_score(y_test, predicted)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[480,  13,   3,   0,   0,   0],\n",
       "       [ 46, 418,   7,   0,   0,   0],\n",
       "       [ 16,  47, 357,   0,   0,   0],\n",
       "       [  0,   0,   0, 404,  87,   0],\n",
       "       [  0,   0,   0,  56, 476,   0],\n",
       "       [  0,   0,   0,   0,   0, 537]])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "confusion_matrix(y_test, predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
